# Deep-Learning_Business-Saturation-Score-Prediction
The objective of this project is to predict a "business score" for each of the USA zipcodes. The exact nature of the "business score" (score column) is not disclosed. It is connected to the business/market saturation of a zipcode. 
## Dataset
The feature raw dataset is called "SOI Tax Stats - Individual Income Tax Statistics 2016" from US Government public data (https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi/). The training and testing dataset are from Kaggle, which can be found here (https://www.kaggle.com/c/applications-of-deep-learningwustl-spring-2019/data/).
## Data Cleaning
For this project, the zip code of 0 and 99999 was removed and then groups are assigned from 1 to 6 with median of 'AGI-sub' . For the features (from N1 to ELDERLY in the original dataset), the assigned group number above multiply groups of people to calculate the total number that can be represented for the zip code. The rest part of data used the “N” column multiply “A” column respectively to calculate the total number for that variable. After fitting the model, we get RMSE about 6.5.
## Modeling
Then we tried to find more data to get better RMSE. Therefore, we used the dataset from US Census Bureau. We drop some columns that we think can be represented by zip code and columns that have only one kind of value, then we fill NAs with zero. Then, we merged the two data sets on zip code to get the final cleaned dataset. After fitting the model, we found this combined dataset can generate a much better RMSE, which is around 1.3. Therefore, we decided to only use the second dataset in the model since it has more effective effect on predicting results.

Finally, we merged the training data set with the cleaned data set from US Census Bureau to train. Before model fitting, we normalized all numerical data into z-score and all categorical data into dummy. Then, we trained our model with deep neural network, which has four hidden layers, and each layer has 200, 200, 100 and 1 nodes respectively. In our model, we applied cross validation through using 5 folds to pick the best model with lowest RMSE (split data into 5 parts, 4 folds for training and 1 for validating with completely combinations), also we used 200 epochs and early stopping for monitoring our model not overfit. We chose 1e-8 as our learning rate at first, then tuning it with smaller number, which we got to 1e-5, and we chose patience equal to 5 and verbose to 1. Finally, we get the public board score of 0.21446 and private board score of 0.7214.
